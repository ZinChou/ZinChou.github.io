# **深度学习推荐系统**

## 2 前深度学习时代

### 2.2 协同过滤  (CF)

- **UserCF**
- **ItemCF**

### 2.3 矩阵分解 (MF)

### 2.4 逻辑回归 (LR)

- **数学公式：**
  $$
  f(x)=\frac{1}{1+e^{-(\boldsymbol{wx+b})}}
  $$

- 基于极大似然估计，目标函数为：
  $$
  L(\boldsymbol{w})=\prod_{i=1}^m{P(y|\boldsymbol{x;}\boldsymbol{w})}=\prod_{i=1}^m{\big(f_\boldsymbol{w}(x)\big)^y\big(1-f_\boldsymbol{w}(x)\big)^{1-y}}
  $$

- 两侧取log，以及乘以系数 $-\frac{1}{m}$ (将求最大值问题转换成求最小值问题) 得：
  $$
  J(\boldsymbol{w})=-\frac{1}{m}\log L(\boldsymbol{w}) = -\frac{1}{m}\sum_{i=1}^{m} \Big[ y_i \log f_{\boldsymbol{w}}(x_i) + (1-y_i) \log \big(1-f_{\boldsymbol{w}}(x_i)\big) \Big].
  $$

- 对 $\boldsymbol{w}_j$ 求偏导：
  $$
  \frac{\partial \ell(\boldsymbol{w})}{\partial w_j} = -\frac{1}{m}\sum_{i=1}^{m} \Big( f_\boldsymbol{w}(\boldsymbol{x}^i)-y^i \Big) \boldsymbol{x}_{j}^i.
  $$

---



### 2.5 因子分解机 (FM)

- **FM**
  $$
  \hat{y} = w_0 + \sum_{i=1}^{n} \boldsymbol{w}_i\, x_i + \sum_{i=1}^{n} \sum_{j=i+1}^{n} \langle \mathbf{v}_i, \mathbf{v}_j \rangle \, x_i \, x_j
  $$
  
- **FFM**

$$
\hat{y} = w_0 + \sum_{i=1}^{n} \boldsymbol{w}_i\, x_i + \sum_{i=1}^{n} \sum_{j=i+1}^{n} \langle \mathbf{v}_{i, f_j}, \mathbf{v}_{j, f_i} \rangle\, x_i \, x_j
$$

### 2.6 GBDT + LR

<img src=".\typora-user-images\GBDT-LR.png" width="400">

- **GBDT vs XGBoost**

  XGBoost 实现并行主要依靠以下几种机制，这些机制共同作用以提高训练速度和扩展性：

  ------

  1.  **特征并行（Feature-level Parallelism）**

     直方图近似算法：

     XGBoost 通过将连续特征离散化成直方图，减少了遍历所有数据点的计算量。

     - **并行构建直方图**：对每个特征，多个线程同时统计直方图信息（例如，每个 bin 内的梯度和 Hessian 累计值）。
     - 这种方式让每个线程负责不同特征或不同区间，从而加速了最佳分裂点的搜索。

  2. **数据并行（Data-level Parallelism）**

     - **多线程计算**：
       - 参数 `nthread` 可以设置 XGBoost 使用的 CPU 线程数。
       - 每个线程在处理同一棵树的不同数据块上进行计算，从而加快梯度和 Hessian 的计算。
     - **分布式训练**：
       - XGBoost 支持分布式环境（如 Hadoop、Spark、MPI），通过数据划分和节点间通信，将大数据集分散到多个机器上并行训练。
       - 每个节点独立构建局部直方图，然后将结果合并以确定全局最佳分裂点。

  3. **并行树构建**

     - **加速分裂点搜索**：
        在每一棵树的构建过程中，XGBoost 会并行地对不同特征进行分裂点评估，然后选出使得目标函数提升最大的那个分裂点。
     - **利用直方图方法降低计算量**，结合并行计算，使得整体树构建时间显著减少。

  4. **GPU 加速**

     - GPU 直方图算法：
       - XGBoost 支持使用 GPU 加速，特别是 `tree_method="gpu_hist"` 模式。
       - GPU 的并行计算能力可以在构建直方图、计算梯度和 Hessian 时大幅加速整个训练过程。

  ------

  

### 2.7 LS-PLM (别名 MLR)

$$
f(\boldsymbol{x})=\sum_{i=1}^{m}\pi_i(\boldsymbol{\boldsymbol{x}})\cdot \eta_i(\boldsymbol{\boldsymbol{x}})=\sum_{i=1}^{m}\frac{e^{\boldsymbol{\mu}_i\boldsymbol{x}}}{\sum_{j=1}^m{e^{\boldsymbol{\mu}_jx}}}\cdot\frac{1}{1+e^{-\boldsymbol{w}_i\cdot \boldsymbol{x}}}
$$

<img src=".\typora-user-images\MLR.png" width="800">

---



## 3 深度学习在推荐系统中的应用

### 3.2 AutoRec

<img src=".\typora-user-images\AutoRec.png" width="400">

✅ **损失函数：**
$$
\min_\theta\sum_{i=1}^n\left \| \boldsymbol{r}^{(i)}-h(\boldsymbol{r}^{(i)};\theta) \right \|_2^2+\frac{\lambda}{2}\cdot(\left \| \mathbf{W} \right \|_F^2+\left \| \mathbf{V} \right \|_F^2)
$$


---



### 3.3 Deep Crossing模型

<img src=".\typora-user-images\DeepCrossing.png" width="400">

✅ Stack Layer: 特征拼接

---



### 3.4 NeuralCF模型

<img src=".\typora-user-images\NeuralCF.png" width="600">

✅ GMF Layer: 对用户嵌入向量 **u**和物品嵌入向量 **i**进行逐元素乘积（element-wise product），生成用户-物品交互特征。

✅ NeuMF Layer: 对用户Embedding向量 **u** 和物品Embedding向量 **i** 进行MLP操作。

✅ **损失函数：**
$$
\text{Loss}_\text{Cross Entropy}=-\sum_iy_i\ln \big( \sigma(x)_i\big)
$$
​	其中 $\sigma(x)_i$ 表示softmax函数对第 $i$ 个分类的预测值。

---



### 3.5 PNN模型

<img src=".\typora-user-images\PNN.png" width="600">

✅**z**: Embedding向量内积(也称为点积)。

- **定义**：
  对于两个向量 $ \mathbf{u} = [u_1, u_2, \dots, u_n]$ 和 $  \mathbf{v} = [v_1, v_2, \dots, v_n] $，它们的内积定义为：
  $$
  \mathbf{u} \cdot \mathbf{v} = \sum_{i=1}^n u_i v_i
  $$

---



✅**p**: Embedding向量外积。

- **定义**：
  对于两个向量 $ \mathbf{u} = [u_1, u_2, \dots, u_m]^T $ 和 $ \mathbf{v} = [v_1, v_2, \dots, v_n]^T $，它们的元素外积定义为：
  $$
  \mathbf{u} \otimes \mathbf{v} = \mathbf{u} \mathbf{v}^T =
  \begin{bmatrix}
  u_1 v_1 & u_1 v_2 & \dots & u_1 v_n \\
  u_2 v_1 & u_2 v_2 & \dots & u_2 v_n \\
  \vdots & \vdots & \ddots & \vdots \\
  u_m v_1 & u_m v_2 & \dots & u_m v_n
  \end{bmatrix}
  $$

---



### 3.6 Wide&Deep 模型

<img src=".\typora-user-images\Wide&Deep.png" width="800">

✅ Cross Product Transformation：对手动筛选的相关特征的Embedding向量进行逐元素相乘。

- **定义**：

  对于两个向量 $ \mathbf{u} = [u_1, u_2, \dots, u_n] $ 和 $ \mathbf{v} = [v_1, v_2, \dots, v_n] $，它们的逐元素相乘结果定义为：
  $$
  \mathbf{w}=\mathbf{u}\odot\mathbf{v}=[u_1v_1,u_2v_2,\dots,u_nv_n].
  $$

  ---

  

#### 3.6.3 Deep&Cross 模型 （DCN）

<img src=".\typora-user-images\Deep&Cross.png" width="800">

### 3.7 FM与深度学习模型的结合

#### 3.7.1 FNN

<img src=".\typora-user-images\FNN.png" width="600">

✅ 利用FM的权重初始化Embedding层

---



#### 3.7.2 DeepFM

<img src=".\typora-user-images\DeepFM.png" width="800">

---



#### 3.7.3 NFM

<img src=".\typora-user-images\NFM.png" width="400">

✅ Bi-Interaction Pooling: 

​	假设 $V_x$是所有特征域的Embedding向量的集合，特征交叉池化层操作如下：
$$
f_{BI}(V_x)=\sum_{i=1}^n\sum_{j=i+1}^n(x_iv_i)\odot(x_jv_j)
$$
​	其中$x_i$为标量，表示输入特征向量$x$的第$i$个特征；$v_i$为向量，表示第$i$个特征对应的Embeddding向量。

---



### 3.8 注意力机制在推荐模型中的应用

#### 3.8.1 AFM

<img src=".\typora-user-images\AFM.png" width="800">

---



#### 3.8.2 DIN

<img src=".\typora-user-images\DIN.png" width="600">

---



### 3.9 DIEN模型——序列模型与推荐系统的结合

<img src=".\typora-user-images\DIEN.png" width="800">

✅ AuGRU不同点：将更新门的结果乘以注意力得分（[GRU友情链接](http://zh.gluon.ai/chapter_recurrent-neural-networks/gru.html)）。

---



#### 3.10 强化学习与推荐系统的结合

#### 3.10.3 DRN

<img src=".\typora-user-images\DRN.png" width="800">

---



## 4 Embedding 技术在推荐系统中的应用

#### 4.4.2 

Graph Embedding中的同质性与结构性：

同质性：距离相近的节点Embedding向量应尽量相似。

结构性：结构上相似的节点的Embedding向量应尽量相似。

---



## 5 多角度审视推荐系统

#### 5.1.2 推荐系统的常用特征

1. 用户行为数据

   - 显性反馈行为：评分，赞，踩；
   - 隐性反馈行为：点击，播放，收藏，购买，加入购物车。

2. 用户关系数据

3. 属性，标签类数据

   - 用户
   - 物品

4. 内容类数据

   标签类数据的衍生，往往是大段的描述型文字、图片。

5. 上下文信息

   - 时间信息
   - 地点信息
   - $\cdots$

6. 统计类特征

   - 历史CTR
   - 历史CVR
   - 物品热门程度
   - $\cdots$

7. 组合类特征

   - 年龄+性别
   - $\cdots$

---

### 5.2 推荐系统召回层主要策略

- 多路召回策略

​	<img src=".\typora-user-images\5.2.png" width="400">

- 基于Embedding的召回方法

---

#### 5.3.3 推荐系统“模型”的实时性

1. 全量更新

2. 增量更新

3. 在线学习

4. 局部更新

5. 客户段更新

   用户Embedding

#### 5.4.2 Entire Space Multi-task Model (ESMM)

<img src=".\typora-user-images\ESMM.png" width="600">

### 5.6 冷启动解决办法

1. 冷启动类型
   - 用户冷启动：新用户注册后，没有历史行为数据时的个性化推荐。
   - 物品冷启动：系统加人新物品后（新的影片、新的商品等），在该商品还没有交互记录时，如何将该物品推荐给用户。
   - 系统冷启动：在推荐系统运行之初，缺乏所有相关历史数据时的推荐。
2. 解决办法
   - 基于规则的冷启动过程
   - 丰富冷启动过程中可以获得的用户和物品特征
   - 利用主动学习，迁移学习和”探索与利用“机制

---



#### 5.7 探索与利用

#### 5.7.1 传统的探索与利用方法

1. $\epsilon$-Greedy

   $\epsilon\in[0,1]$,以 $\epsilon$ 概率随机选择，以 $(1-\epsilon)$ 概率选择平均期望最高的（平均期望由历史选择数据得到）。

2. Thompson Sampling 算法

   每个老虎机对应一beta分布：$\text{beta}_i(\text{win},\text{lose})$，其中win,lose分别对应是收益为1，0的次数（假设收益是二值分布）。

   每个老虎机对应beta分布产生一个随机数，选择最大随机数对应老虎机。

3. UCB 算法

​	每次选择UCB值最大的老虎机，UCB值定义如下：
$$
\text{UCB}(j)=\widetilde{x}_j+\sqrt{\frac{2\text{log}t}{n_j}}
$$
​	其中 $\widetilde{x}_j$ 表示老虎机 j 利益的初始化经验期望，t表示至今摇臂总次数，$n_j$ 表示第 j 个老虎机至今摇臂的次数。

---



#### 5.7.2 个性化的探索与利用方法 (LinUCB)

​	LinUCB 的探索部分得分定义:
$$
\alpha\sqrt{x_{t,a}^T\boldsymbol{A}^{-1}x_{t,a}}
$$
​	其中 $\alpha$ 表示控制探索强弱力度的超参数, $x_{t,a}$ 代表老虎机 a 在第 $t$ 次试验中的特征向量, 矩阵 $\boldsymbol{A}$ 定义如下:
$$
\boldsymbol{A}_a\stackrel{\text{def}}{=}\boldsymbol{D}_a^T\boldsymbol{D}_a+\boldsymbol{I}_d
$$
​	其中矩阵 $\boldsymbol{D}$ 是一个 $m\times d$ 维的样本矩阵。$m$ 指的是训练样本中与老虎机 a 相关的 $m$ 个训练样本。$\boldsymbol{I}_d$ 是单位矩阵。

---



## 6 深度学习推荐系统的工程实现

### 6.1 推荐系统的数据流

#### 6.1.1 批处理大数据结构

- 分布式存储 + Map Reduce
- 延迟较大（小时级别）

<img src=".\typora-user-images\6.1.1.png" width="800">

**Map Reduce** 是一种 **分布式计算框架**，用于 **大规模数据处理**。它最早由 **Google** 在 2004 年提出，并在 **Hadoop** 生态中被广泛应用。

##### **📌 Map Reduce 适用于：**

- **大规模数据处理**（如 TB 级、PB 级数据）
- **并行计算**（如日志分析、搜索索引构建）
- **分布式架构**（如 Hadoop 集群）
- **流式数据处理**（如 ETL 任务）

✅ **核心思想**：将 **大规模计算任务拆分成多个子任务**，分别进行计算，最后汇总结果。

##### **📌 MapReduce 计算过程**

1. **输入数据拆分**：大数据被分割成多个小块（HDFS 切片）。
2. Map 阶段（映射）：
   - 读取数据，按 **key-value** 形式处理。
   - 对数据进行 **映射（mapping）**，产生中间键值对。
3. Shuffle 阶段（洗牌 & 排序）：
   - Map 输出的数据 **按 key 分组**，相同 key 的数据发送到相同的 Reduce 任务。
   - 对 key 进行排序，保证 Reduce 处理时是有序的。
4. Reduce 阶段（归约）：
   - 对相同 key 的数据进行 **聚合、汇总** 计算，生成最终结果。
5. **结果输出**：将 Reduce 计算后的数据存储到 **HDFS 或数据库**。

------



#### 6.1.2 流计算大数据结构

- 分钟级延迟

<img src=".\typora-user-images\6.1.2.png" width="800">

---



#### 6.1.3 Lambda 架构

- 商业界以此为主

<img src=".\typora-user-images\6.1.3.png" width="800">

---



#### 6.1.4 Kappa 架构

<img src=".\typora-user-images\6.1.4.png" width="800">

---



#### 6.1.5 大数据平台与推荐系统的整合

1. 以HDFS为代表的离线海量数据存储平台，主要负责存储离线训练用的训练样本。
2. 以Redis为代表的在线实时特征数据库，主要负责为模型的在线服务提供实时特征。

<img src=".\typora-user-images\6.1.5.png" width="800">

---





## 7 推荐系统的评估

### 7.1 离线评估方法与基础指标

#### 7.1.1 离线评估主要方法

- **Holdout 检验**

  将原始样本集合随机划分为训练集和测试集

- **交叉验证**

  - k-flod 交叉验证：将样本分为 k 个大小一样的子集，每次将一个子集当测试集，其他当训练集
  - 留一验证：k = n, 其中 n 为样本总个数

- **自助法**

  自助法（Bootstrap）是基于自助采样法的检验方法：对于总数为n的样本集合，进行n次有放回的随机抽样，得到大小为n的训练集。

  在n次采样过程中，有的样本会被重复采样，有的样本没有被抽出过，将<u>这些没有被抽出的样本</u>作为<u>验证集</u>进行模型验证，就是自助法的验证过程。

---



#### 7.1.2 离线评估指标

- 准确率

- 精确度和召回率

- 均方根误差

- 对数损失函数
  $$
  \text{LogLoss} = -\frac{1}{N}\sum_{i=1}^N {\big(y_i\text{log}P_i +(1-y_i)\text{log}(1-P_i)\big)}
  $$
  其中，$y_i$ 为输人实例 $x_i$ 的真实类别，$P_i$为预测输人实例 $x_i$ 是正样本的概率，$N$ 为样本总数。

---

 

### 7.2 直接评估推荐系统序列的离线指标

#### 7.2.1 P-R 曲线

- 纵坐标：Precision = $\frac{TP}{TP+FP}$
- 横坐标：Recall = $\frac{TP}{TP+FN}$
- 曲线上的点表示：在某一阈值下，模型将大于该阈值的结果判定为正样本，将小于该阈值的结果判定为负样本时，排序结果对应的召回率和精确率。
- **AUC** (Area under Curve)越大越好。

#### 7.2.1 ROC 曲线

- 纵坐标：真阳性率 TPR = $\frac{TP}{TP+FN}=\frac{TP}{P}$
- 横坐标：假阳性率 FNR =  $\frac{FP}{TN+FP}=\frac{FP}{N}$

#### 7.2.2 平均精度均值

- mean Average Precision (mAP), 对所有客户的平均精度取均值。

---

### 7.3 Replay

​	动态离线评估方法

<img src=".\typora-user-images\7.3.png" width="600">

---



### 7.4 A/B测试与线上评估

无可替代的原因：

- 离线评估无法完全消除数据有偏现象的影响
- 离线评估无法完全还原线上的工程环境
- 线上系统的某些商业指标在离线评估中无法计算	

#### 7.4.2 A/B测试的 “分桶” 原则

- **层与层之间流量 “正交”**

  <img src=".\typora-user-images\7.4.2.png" width="300">

- **同层之间的流量 “互斥”**

  - 如果同层之间进行多组A/B测试，那么不同测试之间的流量是不重叠的，即“互斥”的。
  - 一组A/B测试中实验组和对照组的流量是不重叠的，是“互斥”的。

### 7.5 Interleaving

- **将多个模型的输出混合在一起（Interleaved Ranking）**：

​	例如，在搜索推荐中，我们可以把 **模型 A 和模型 B 的结果交错展示**，而不是分别分配用户流量。

- **根据用户的交互反馈，动态判断模型效果**

<img src=".\typora-user-images\7.5.png" width="500">

---



#### 8.1.4 降采样和模型校正

​	**负采样**：保留全量正样本，对负样本就行降采样。

​	负采样会造成CTR预估值漂移，需要对CTR预估值就行校正，校正公式：
$$
q=\frac{p}{p+(1-p)/w}
$$
​	其中 $q$ 是校正后的CTR, $p$ 是模型预估的CTR, $w$ 是负采样频率。 
